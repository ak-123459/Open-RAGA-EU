from langchain_core.runnables import Runnable, RunnableConfig
from typing import Any, Dict, Optional
from langchain_core.runnables.utils import Input, Output
from generation.prompt_templates import get_template
from langchain_core.output_parsers import StrOutputParser
from langchain.chains import LLMChain
from langchain.chains import create_history_aware_retriever
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.runnables import RunnableLambda, RunnableMap
import os
import yaml
from langchain.memory import ConversationBufferWindowMemory
from pathlib import Path
import re

# Get the root path
root_path = os.path.dirname(os.path.abspath(__file__))



# Load a .yaml or .yml file
with open(Path(root_path).parent/"generation/prompt_templates.yaml", "r",encoding="utf-8") as file:
    prompt_config = yaml.safe_load(file)


translate_prompt_Temp = get_template('translator')['translator']
history_aware_prompt = prompt_config['context_aware_prompt']['prompt']
details_agent_prompt = prompt_config['details_agent_prompt']['prompt']
translate_to_hindi = get_template('translator_english_hindi')['translator_english_hindi']

# Chat manager class to manage chat
class ChatManager:
""" 
this chat manager class will handle vector database and llm with langchain this class create the translation chain,history_aware_retriever_chain and  q_a_chain
with langchain.
"""
    def __init__(self, llm, vector_db):

        self.llm = llm
        self.vector_db = vector_db
        self.rag_chain = self._create_rag_chain()


    
     # Create a rag chain
    def _create_rag_chain(self):
    """this will create a combine chain :
    first it will retrieve the similar context then it will pass to the qa chain in which llm will use this retrieved context for question answer.
    """
        return create_retrieval_chain(
            self.history_aware_retriever_chain(),
            self.q_a_chain()
        )



    # history aware/context aware chain for retrieving the context from the vector database
    def history_aware_retriever_chain(self):
    """ 
    this function will take the chat history,user query and history_aware_prompt to create a standalone question then this will pass to the history_aware_retriever_chain
    chain that will retrieve the simlilar context.   
    Here, it initializes a retriever chain that:
        1.Converts user question using chat history.
        2.Uses that question to search the vector DB (semantic search).
        3.Returns relevant documents.
    """


       # ChatPromptTemplate to pass list of args or prompts to create a standlone question 
        contextualize_q_prompt = ChatPromptTemplate.from_messages(
            [
                ("assistant", history_aware_prompt),
                MessagesPlaceholder("chat_history"),
                ("user", "{input}"),
            ]
        )


        # retrieving the context using similar to the question
        history_aware_retriever_chain = create_history_aware_retriever(
            self.llm, self.vector_db, contextualize_q_prompt
        )

        return history_aware_retriever_chain




    # question answer chain for the answer  the  questions from the retrieved context
    def q_a_chain(self):
    """ 
    this qa chain will use to answer the question this is a details agent chain this will generate the final answer.
    This qa_prompt:
        1.Sets the assistantâ€™s behavior (e.g., respond formally or with details).
        2.Includes the chat history, so answers can be aware of ongoing conversation.
        3.Uses the user's input as the main query to be answered.
        
    """
        
        qa_prompt = ChatPromptTemplate.from_messages(
            [
                ("assistant", details_agent_prompt),
                MessagesPlaceholder("chat_history"),
                ("user", "{input}"),
            ])

        details_agent_chain = create_stuff_documents_chain(self.llm, qa_prompt)

        return details_agent_chain


   # combine all steps in run function that will takes query and return the reponse
    async def run(self,query,chat_history)->Any:
        """
        process query and  combine all chains.
    
        Args:
        query(str): query/question asked by user
        chat_history(list): user chat history
    
        Returns:
          response/answer generated by llm
        
        """

        # translate the text from romanized hindi to english
        translated = (translate_prompt_Temp | self.llm ).invoke({'query': query})

        # get the actual translations
        translated_text = translated.content.strip()

        # call the history_aware_retriever_chain  and q_a chain
        result = self.rag_chain.invoke({"input": translated_text, "chat_history": chat_history })

        # translate from english to hindi   
        translated = ( translate_to_hindi | self.llm).invoke({'query': result['answer']})
        
        print("----translated---",translated.content.strip())
        
        return  translated.content.strip()



